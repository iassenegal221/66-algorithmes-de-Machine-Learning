## Institut des Algorithmes du S√©n√©gal

## 66 algorithmes de Machine Learning

[**Linkdin**](https://www.linkedin.com/company/71517780/)
[**Facobook**](https://www.facebook.com/IASSenegal)
[**Web**](https://www.ias.sn/formations/home)

**üí° Algorithme 1 : Analyse des composants principaux (Principal Component Analysis PCA)**: 
- PCA est une technique de r√©duction de la dimensionnalit√© qui permet d'identifier les corr√©lations et les mod√®les dans l'ensemble de donn√©es afin qu'il puisse √™tre transform√© en un ensemble de donn√©es de dimensions nettement inf√©rieures sans aucune perte d'informations importantes. Il s'agit d'une technique statistique non supervis√©e utilis√©e pour examiner les interrelations entre un ensemble de variables. Elle est √©galement connue sous le nom d'analyse factorielle g√©n√©rale o√π la r√©gression d√©termine une ligne de meilleur ajustement. Cela fonctionne √† condition que, bien que les donn√©es dans un espace de dimension sup√©rieure soient mapp√©es sur des donn√©es dans un espace de dimension inf√©rieure, la variance ou la propagation des donn√©es dans l'espace de dimension inf√©rieure doit √™tre maximale.
- L'algorithme PCA est r√©alis√© selon les √©tapes suivantes :

      1. Standardization of Data
      
      2. Computing the covariance matrix
      
      3. Calculation of the eigenvectors and eigenvalues
      
      4. Computing the Principal components
      
      5. Reducing the dimensions of the Data.
    
- Reference:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - [**ML from Scratch on Youtube**](https://lnkd.in/gNPM6vW2) 


**üí° Algorithme 2 : Principal Component Analysis using scikit-learn**: 
- L'algorithme PCA projette des observations sur les composants principaux de la matrice de caract√©ristiques qui conservent le plus de variance. PCA peut √©galement √™tre utilis√©e dans le sc√©nario, o√π nous avons besoin de conserver des fonctionnalit√©s qui partagent une variance maximale. PCA est impl√©ment√© dans scikit-learn en utilisant la m√©thode PCA¬†:

      class sklearn.decomposition.PCA(n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)

- Le parametre n_components a deux op√©rations, selon l'argument fourni. Si l'argument est sup√©rieur √† 1, n_components renverra autant de fonctionnalit√©s. Si l'argument de n_components est compris entre 0 et 1, PCA renvoie le nombre minimum d'entit√©s qui conservent autant de variance. Il est courant d'utiliser des valeurs de 0,95 et 0,99, ce qui signifie que 95¬†% et 99¬†% de la variance des caract√©ristiques d'origine ont √©t√© conserv√©es.
- whiten =True transforme les valeurs de chaque composante principale afin qu'elles aient une moyenne nulle et une variance unitaire. Le blanchiment supprimera certaines informations du signal transform√© mais peut parfois am√©liorer la pr√©cision pr√©dictive des estimateurs en aval.
- svd_solver=" randomized", qui impl√©mente un algorithme stochastique pour trouver les premi√®res composantes principales en souvent beaucoup moins de temps.

- R√©f√©rence:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - [**Scikit-learn Implementation**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) 
